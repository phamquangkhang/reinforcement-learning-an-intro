{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 10: On-policy Control with Approximation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Episodic Semi-gradient Control\n",
    "- **Goal**:To approximate action value function __$\\hat{q}* \\approx q_\\pi$__\n",
    "- **Gradient-update**:<br>\n",
    "$\n",
    "\\begin{align}\n",
    "\\boldsymbol{w}_{t+1} = \\boldsymbol{w}_t + \\alpha[U_t - \\hat{q}(S_t, A_t, \\boldsymbol{w}_t)]\\bigtriangledown\\hat{q}(S_t, A_t, \\boldsymbol{w}_t)\n",
    "\\end{align}\n",
    "$\n",
    ", $U_t$: update target\n",
    "- In Sarsa Case:<br>\n",
    "$\n",
    "\\begin{align}\n",
    "\\boldsymbol{w}_{t+1} = \\boldsymbol{w}_t + \\alpha[R_{t+1} + \\gamma\\hat{q}(S_{t+1}, A_{t+1}, \\boldsymbol{w}_{t+1}) - \\hat{q}(S_t, A_t, w_t)]\\bigtriangledown\\hat{q}(S_t, A_t, \\boldsymbol{w}_t)\n",
    "\\end{align}\n",
    "$\n",
    "- To form **control methods**, we need to couple such action-value prediction methods with techniques for policy improvement and action selection. Suitable techniques applicable to continuos actions, or to actions from large discrete sets, are a topic of ongoing research as yet no clear resolution.\n",
    "- **Pseudo-code** for Semi-gradient Sarsa:<br>\n",
    "    <img src='./images/chap10/chap10_1_ Sarsa Pseudo code.png' style='width: 600px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 10.2 Semi-gradient n-step Sarsa\n",
    "- **_n_-step return**:<br>\n",
    "$\n",
    "\\begin{align}\n",
    "G_{t:t+n} &= R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1}R_{t+n} + \\gamma^n\\hat{q}(S_{t+n}, A_{t+n}, \\boldsymbol{w}_{t+n-1}),  t+n<T\\\\ \\\\\n",
    "\\boldsymbol{w}_{t+n} &= \\boldsymbol{w}_{t+n-1} + \\alpha [G_{t:t+n} - \\hat{q}(S_t, A_t, \\boldsymbol{w}_{t+n-1}]\\bigtriangledown\\hat{q}(S_t, A_t, \\boldsymbol{w}_t)\n",
    "\\end{align}\n",
    "$\n",
    "- **Pseudo-code** for Semi-gradient n-step Sarsa:<br>\n",
    "    <img src='./images/chap10/chap10_2_ n-step Sarsa Pseudo code.png' style='width: 600px'>\n",
    "- This algorithm tends to learn **faster at n=8 than at n=1** on Mountain Car task:<br>\n",
    "    <img src='./images/chap10/chap10_fig10-3.png' style='width: 700px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Average Reward: A New Problem Setting for Continuing Tasks\n",
    "+ **Average reward**:\n",
    "    - Applies to continuing problem\n",
    "    - No discounting\n",
    "    - Commonly for dynamic programming but less in reinforcement learning\n",
    "    - Better setting than discount for function approximation (see 10.4)\n",
    "+ **Quality of policy $\\pi$**: average rate of reward<br>\n",
    "    $\n",
    "    \\begin{align}\n",
    "    r(\\pi) &= \\lim_{h\\to\\infty} \\frac{1}{h}\\sum\\limits_{t=1}^{h} \\mathbb{E}[R_t | S_0, A_{0:t-1}\\sim\\pi] \\\\\n",
    "           &= \\lim_{t\\to\\infty} \\mathbb{E}[R_t | S_0, A_{0:t-1}\\sim\\pi]\\\\\n",
    "           &= \\sum_{s}\\mu_{\\pi}(s)\\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)r\n",
    "    \\end{align}\n",
    "    $\n",
    "+ **$\\mu_{\\pi}$** is the steady-state distribution, which means:<br>\n",
    "    $\n",
    "    \\begin{align}\n",
    "    \\sum_{s}\\mu_{\\pi}(s)\\sum_{a}\\pi(a|s)p(s'|s,a) = \\mu_{\\pi}(s')\n",
    "    \\end{align}\n",
    "    $\n",
    "+ **Differential return**:<br>\n",
    "    $\n",
    "    \\begin{align}\n",
    "    G_t = R_{t+1} - r(\\pi) + R_{t+2} - r(\\pi) + R_{t+3} - r(\\pi) + ...\n",
    "    \\end{align}\n",
    "    $\n",
    "+ **Bellman equations** with differential return: <br>\n",
    "    $\n",
    "    \\begin{align}\n",
    "    v_{\\pi} &= \\sum_{a}\\pi(a|s)\\sum{r,s'}p(s',r|s,a)[r-r(\\pi)+v_{\\pi}(s')],\\\\\n",
    "    q_{\\pi}(s,a) &= \\sum_(r,s')p(s',r|s,a)[r-r(\\pi)+\\sum_{a'}\\pi(a'|s')q_{\\pi}(s',a'),\\\\\n",
    "    v_*(s) &= \\max_{a}\\sum_{r,s'}p(s',r|s,a)[r-\\max_{\\pi}r(\\pi)+v_*(s')],\\\\\n",
    "    q_*(s,a) &= \\sum{r,s'}p(s',r|s,a)[r - \\max_{\\pi}r(\\pi) + \\max_{a'}q_*(s',a')]\n",
    "    \\end{align}\n",
    "    $\n",
    "+ **Differential form of TD errors:**<br>\n",
    "    $\n",
    "    \\begin{align}\n",
    "    \\delta_t &= R_{t+1} - \\overline{R}_{t+1} + \\hat{v}(S_{t+1}, \\boldsymbol{w}_t -\\hat{v}(S_t,\\boldsymbol{w}_t)\\\\\n",
    "    \\delta_t &= R_{t+1} - \\overline{R}_{t+1} + \\hat{q}(S_{t+1}, A_{t+1}, \\boldsymbol{w}_t -\\hat{q}(S_t,A_t,\\boldsymbol{w}_t)\n",
    "    \\end{align}\n",
    "    $\n",
    "+ **Update rule:**<br>\n",
    "    $\n",
    "    \\begin{align}\n",
    "    \\boldsymbol{w}_{t+1} = \\boldsymbol{w}_t + \\alpha\\delta_t\\bigtriangledown\\hat{q}(S_t, A_t, \\boldsymbol{w}_t)\n",
    "    \\end{align}\n",
    "    $\n",
    "+ **Pseudo-code for differential semi-gradient Sarsa**<br>\n",
    "    <img src='./images/chap10/chap10_3_differential Sarsa.png' style='width: 600px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Deprecating the Discounted Setting\n",
    "- Discounted reward might not work well in approximate case.\n",
    "- **Setup:** an infinite sequence of returns with no beginning or end. All the feature vectors of state may be the same.\n",
    "- **Discounted return** must be done through a large-time interval.\n",
    "- **Discounted return** via large-time interval is propotional to average reward.<br>\n",
    "    <img src='./images/chap10/chap10_4_Futility of discounting in continuing problems.png' style='width: 600px'>\n",
    "- However, average-reward and function approximation **are not guaranteed to improve the policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Differential Semi-gradient _n_-step Sarsa \n",
    "+ **_n_-step version of TD error:**<br>\n",
    "    $\n",
    "    \\begin{align}\n",
    "    G_{t:t+n} &= R_{t+1} - \\overline{R}_{t+1} + R_{t+2}  - \\overline{R}_{t+2} + ... + R_{t+n}  - \\overline{R}_{t+n}+ \\hat{q}(S_{t+n}, A_{t+n}, \\boldsymbol{w}_{t+n-1})\\\\\n",
    "    \\delta_t &= G_{t:t+n} - \\hat{q}(S_t, A_t, \\boldsymbol{w})\n",
    "    \\end{align}\n",
    "    $\n",
    "+ **Pseudo-code for differential semi-gradient _n_-step Sarsa:**<br>\n",
    "    <img src='./images/chap10/chap10_5_Differential semi-gradient n-step Sarsa.png' style='width: 600px'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
